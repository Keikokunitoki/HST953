---
title: "HST.953 Workshop: Exploratory Data Analysis "
author: "Your Name Here"
date: "Oct 4, 2019"
output: 
  html_document: 
    fig_height: 8
    fig_width: 10
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Instructions:

**Before beginning**, please test to see if the Rmd file will compile on your system by clicking the "Knit HTML button" in R studio above.  If it does not compile, seek out help during the Office Hours, on Slack, or online.  Chances are, you are missing a component necessary to have `rmarkdown` compile on your system.  We would be very interested in: 1) What was wrong?, 2) How you fixed it? and  3) What resources did you use?  This will help us improve the workshops for future years of this class.

!["Where To Click"](https://jraffa.github.io/images/knithtml.png)

**How This Workshop Works**: There is some text, followed by relevant examples with code and output, followed by some *student questions*.  The intention is for students to complete the student questions.  You are not however required to hand the workshop in.  We would recommend that you complete this by next week, as it will give you a solid foundation in some concepts related to `R`, `rmarkdown` and exploratory data analysis.

*To complete the workshop, fill in necessary code in the places indicated with* `# Students: Insert your code here` and text based answers `### Student Answer`


# Principles of Exploratory Data Analysis (EDA)

EDA's goal is to better understand the data and the process by which it was 
generated.  

Within statistics, it is largely considered separate from inferential/confirmatory statistics (e.g., hypothesis testing, point and interval estimates, etc), where EDA has a very diverse and important set of goals:

- Provide an opportunity to do additional data cleaning.
- Understand how the data is generated, and what the relationships between variables may be.
- Suggest questions and hypotheses that can be subsequently answered and tested.
- Identify what statistical methods may be most appropriate for the data to follow up with these questions and hypotheses.

EDA was coined, developed and advocated for by John Tukey.  His book, entitled _"Exploratory Data Analysis"_ was published in 1977, and is still in use today.  It may seem like an oddity, but it was a fundamental change in how data science / statistics was done.  Fundamentally he sums up EDA with this quote:

_"It is important to understand what you CAN DO, before you learn to measure how WELL you seem to have DONE it."_
_-- J. W. Tukey (1977)_

If you don't understand the data, it becomes difficult to know how to analyze it.  Confirmatory and exploratory analyses are not superior or inferior to one another, rather they are complementary.  With all the tools available to do both, ignoring one of them is inexcusable.

_"Today, exploratory and confirmatory (analysis) -- can -- and should -- proceed side by side."_
_-- J. W. Tukey (1977)_



## Cognitive Disfluency -- make it work for you?

There is often an urge due to productivity, laziness, or other factors to plow through with an analysis, using sophisticated analysis techniques to find the results you are looking for.  With the proliferation of large datasets, this can be quite ineffective, as it largely separates the analyst from the data, resulting in misunderstanding or not understanding the data at all.

There is some evidence that *cognitive disfluency* (making it harder to learn) can lead to deeper learning.  For analysts and data scientists this means slowing things down,  often using basic (and sometimes tedious) methods to integrate the primary structure and relationships contained in the data, before pulling out the heavy machinery of modern data analysis.

See: _Alter, A.L., 2013. The benefits of cognitive disfluency. Current Directions in Psychological Science, 22(6), pp.437-442._

All too often the success/failure of an analysis is determined from a single number, when in reality, understanding the data should be the goal.

When working with a new dataset I (Jesse) almost always start with what is discussed below.  While cognitive disfluency deals with difficulty to learn, I believe slowing the analysis down, doing it carefully and critically leads to higher quality analysis.

# Prerequisites

Let's start by reloading the data into a data frame called `dat`:

```{r}
# If the following command doesn't work for you, please run file.choose(), find the aline dataset and replace what it outputs for "aline-dataset.csv" below:
dat <- read.csv("aline-dataset.csv")

```


It is very important you understand how to load CSV files, as often this is the easiest way to load the data when not using `bigrquery`.  As noted in the code, using the `file.choose()` function can be useful for identifying the path to the CSV file.  You should run it in the console.  If you get an error 



# Numerical Forms of EDA

One form of EDA is to provide numerical summaries of the dataset.  This can have many purposes:

- To verify the dataset you loaded is the one you think you did.
- To quantify characteristics of the dataset which need to be reported numerically.


## The `summary` function in `R`

`R` has a very handy function, which performs differently for depending on the type of data structures you apply it to.  This is the `summary` function, and it provides a very useful data summary of data frames.  This comes in the form of five-number summaries (plus the mean) (min-Q1-mean-median-Q3-max) for numeric data and counts for categorical (factor) data.  Summary also works on many types of objects in `R`, and when you don't know what to do with an `R` object (`obj`), it is often good to try `summary(obj)`.

```{r}
summary(dat)
```

As you can see, this function is very verbose, but produces some useful output.  At this point, it's also a good idea to verify the number of rows and columns are correct:

```{r}
nrow(dat)
ncol(dat)
```

This should be what you're expecting (`2751` and `50`).  If it's not, this could indicate a loading error, or a problem with the data extraction.

As you will note, many of the `flg` variables listed in the summary output above, are constrained by 0 and 1.  This is because they have a binary encoding (usually 1 if present, and 0 if not).  Although not necessary in this particular instance, it it sometimes useful to encode these types of variables as factors.  The function `convert.bin.fac` will do this, and we'll use it to create a new data frame called `dat2`, and call the summary function on the new data frame.  We first need to load/install two packages (`devtools` and `MIMICbook`).

```{r include=TRUE}
if(!("devtools" %in% installed.packages()[,1])) {
 install.packages("devtools",repos="https://cloud.r-project.org")
}
library(devtools)
if(!("MIMICbook" %in% installed.packages()[,1])) {
 install_github("jraffa/MIMICbook")
}

library(MIMICbook)
```


The `MIMICbook` package provides some useful functions written for the textbook that we will use throughout some of the workshops for HST.953.  It installs via GitHub.

```{r}
dat2 <- convert.bin.fac(dat)
summary(dat2)
```

As you can now see, instead of means (which under the old encoding equate to proportions of patients where the variable == 1), now we have counts of patients with each *level* of the variable.  This is because `R`'s summary function treats factors and numerical values differently.

Often, you will want to report these summaries separately for different groups.  For instance, is the mean or median age the same for those who received aline, and those who didn't?  A multi-purpose function called `tapply` can help us with this.

```{r}
tapply(dat2$age,dat2$aline_flg,summary)
```

This function stratifies the first argument (`age`) by the second argument (`aline_flg`) and run the third argument (`summary`) on it.  So, in our case, run the `summary` function on `age` for those who received aline (`aline_flg` = 1) and those who didn't (`aline_flg` = 0).

### Student Question 1:

 > a) Using the `dat2` data frame, run the summary function for `sofa_first`, and `service_unit` separately for those with an aline, and those without.  
 > b) Then run the summary function for `age` `sofa_first`, and `service_unit` separately for those who died within 28 days, and those who survived.
 > c) In three (3) sentences or less, discuss these findings, and note anything interesting.
 
 
```{r}
tapply(dat2$sofa_first, dat2$aline_flg, summary)
tapply(dat2$service_unit, dat$aline_flg, summary)
```
b.
```{r}
tapply(dat2$age,dat2$day_28_flg,summary)
tapply(dat2$sofa_first,dat2$day_28_flg,summary)
tapply(dat2$service_unit,dat2$day_28_flg,summary)
```
c.
there seems no significant difference in SOFAscore between patients with and without Aline. 28-day survival rate seems lower among Patient in trauma unit.



### Student Answer 1:

Answer here.

```{r include=FALSE}
tapply(dat2$sofa_first,dat2$aline_flg,summary) # a)
tapply(dat2$service_unit,dat2$aline_flg,summary)

tapply(dat2$age,dat2$day_28_flg,summary) #b)
tapply(dat2$sofa_first,dat2$day_28_flg,summary)
tapply(dat2$service_unit,dat2$day_28_flg,summary)

```

## Producing a Table One and Other Tables
 
The output from summary is very useful, but is generally not acceptable formal research reports, let alone a published paper.  There are several ways to produce a publication which has a better layout.  One way is described in the text book (Chapter 15), another, which we will cover here is through an `R` package called `tableone`.  

As some of you may know, "Table 1" often refers to the table presented in most medical manuscripts which contains information used to describe the cohort.  This often includes information about average patient age, gender distribution, and other important demographic, clinical and socioeconomic characteristics.  We will cover briefly how to use the `CreateTableOne` function in this package to generate a table which is closer to being publication worthy.

The following code will install the `tableone` package and load it.

```{r}
 if(!("tableone" %in% installed.packages()[,1])) {
 install.packages("tableone",repos="https://cloud.r-project.org")
 }
 library(tableone)
```

Here is an example functional call to `CreateTableOne`, which computes either the mean and standard deviation for numeric variables, or count and percentage for factors. You specify which variables you want to include in the table

```{r}
CreateTableOne(vars=c("age","service_unit","aline_flg","day_28_flg"),data=dat2)
```
 


We may want to breakdown these summaries further, like we did above with `tapply`, but we can do it with one function with the `CreateTableOne` function by passing the `strata` parameter.  `strata` specifies which variable to stratify (breakdown) the others by.  For example, here is the same table in the previous chunk, broken down by whether a patient received an aline or not.
 
```{r}
CreateTableOne(vars=c("age","service_unit","aline_flg","day_28_flg"),strata="aline_flg",data=dat2,test=FALSE)
```
 
 
### Student Question 2:

 > a) Compute a Table to summarise those variable considered before (`age`, `service_unit`, `aline_flg` and `day_28_flg`) in addition to `gender_num` and `chf_flg`, but now stratify by survival at 28 days (`day_28_flg`).
 > b) In 1-2 sentences, do you notice anything interesting or peculiar.
 > c) Repeat part a), but now use the `dat` data frame instead of `dat2`.  Briefly discuss the differences and which you prefer.
 
a)
```{r}

CreateTableOne(vars=c("age","gender_num","service_unit","aline_flg","chf_flg"),strata="day_28_flg",data=dat2,test=TRUE)

```
b)
There are significant difference in age and gender between those who survived 28 days or not. CHF or A line use are not different.
c)
```{r}
CreateTableOne(vars=c("age","gender_num","service_unit","aline_flg","chf_flg"), strata="day_28_flg",data=dat)
```
 if we don't use convert.bin.fac, they calculate the percentage not actual count for binary variables. I'd prefer actual number.
 
 
```{r include=FALSE}
CreateTableOne(vars=c("age","service_unit","aline_flg","day_28_flg","gender_num","chf_flg"),strata="day_28_flg",data=dat2,test=FALSE)
CreateTableOne(vars=c("age","service_unit","aline_flg","day_28_flg","gender_num","chf_flg"),strata="day_28_flg",data=dat,test=FALSE)
``` 

### Student Answer 2:

Answer here.


#### Optional:

> As an aside, the following code may help for your projects, as it improves the presentation of the tables above.  You will still need to update the column and row names manually, but this should paste nicely into Word or LateX!

```{r warning=FALSE, message=FALSE}
 if(!("dplyr" %in% installed.packages()[,1])) {
 install.packages("dplyr")
 }
library(dplyr)
CreateTableOne(vars=c("age","service_unit","aline_flg","day_28_flg"),strata="aline_flg",data=dat2,test=FALSE) %>% print(
  printToggle      = FALSE,
  showAllLevels    = TRUE,
  cramVars         = "kon"
) %>% 
{data.frame(
  variable_name             = gsub(" ", "&nbsp;", rownames(.), fixed = TRUE), ., 
  row.names        = NULL, 
  check.names      = FALSE, 
  stringsAsFactors = FALSE)} %>% 
knitr::kable()


```



## Other Bivariate Numerical Summaries

Sometimes you may wish to display the relationships between two or more variables directly.  For categorical variables this can be tricky.  One common way to explore relationships between categorical variables is by producing the cross tabulated tables ("crosstabs" for short).  This is mainly done via the `table` function, which can take several categorical variables, and produce the number of patients which meet criteria for those variables.  For instance, looking at how aline was used in men and women:

```{r}
table(dat2$gender_num,dat2$aline_flg,dnn=c("Gender","Aline"))
```

we can see that aline was used 831 times in men (`gender_num=1`) and 762 times in women (`gender_num=0`).  The raw numbers are often difficult to compare, so often the proportions are more useful.  Applying `prop.table` to our existing table, and adding the argument 1 (for by row, use 2 for columns), we get the proportion of men and women who had aline (48% vs. 52%).


```{r}
prop.table(table(dat2$gender_num,dat2$aline_flg,dnn=c("Gender","Aline")),1)

```

A different summary for the relationships exists when dealing bivariate numeric data.  Sometimes it's desirable to present the strength of the relationship between two variables, and correlation coefficient is the way to go about this.  There is a `cor` function in `R`, but when dealing with only two variables, it's easiest to use the `cor.test` function.  Under the defaults, it computes the Pearson product-moment correlation and computes a hypothesis test (covered briefly earlier today) to assess if there's evidence that the correlation is not zero.  Alternative forms of correlation are computed below, including Spearman's rho and Kendall's tau.  These latter methods are useful when dealing with data which is not necessarily numeric but ordered (e.g., likert based rankings on a 1-5 scale) or has outliers.  Spearman's rho and Kendall's tau are rank based methods, and also have a certain degree of robustness to outliers in the data.  None of these methods are robust to non-linear relationships, and it's very easy to miss a strong relationship between two variables if you rely on these methods in isolation.


```{r}
cor.test(dat2$bun_first,dat2$creatinine_first)
cor.test(dat2$bun_first,dat2$creatinine_first,method="spearman")
cor.test(dat2$bun_first,dat2$creatinine_first,method="kendall")
```

We can produce a scatterplot of the same two variables:

```{r}
with(dat2,plot(bun_first,creatinine_first))
```

We can see that there is indeed a positive correlation between the two variables, but the data has more variability for higher values of `bun_first` and `creatinine_first`.  It's advisable to consider transformations of these two variables and be wary about using Pearson's correlation.

Going beyond two dimensions can be a little tricky.  Plotting on a three dimensional axis, while possible, is not ideal, and I know very few people who can see in four dimensions.

What is possible, is to use other aspects of the plot (e.g., size, color, shape, hue, transparency, location) to identify features you would like to see.  For instance, in the above plot, we can add color to identify those who died:

```{r}
with(dat2,plot(bun_first,creatinine_first,col=day_28_flg,pch=19))
```



## Creating Categorical Variables from Continuous/Numeric Variables
 
Sometimes numeric variables need to be broken down into categorical variables or factors.  This can be done for a variety of reasons.  There is a useful function called `cut2` in the `Hmisc` package.  We install it and use it below.
 
```{r warning=FALSE, message=FALSE} 
 if(!("Hmisc" %in% installed.packages()[,1])) {
 install.packages("Hmisc",repos="https://cloud.r-project.org")
 }
library(Hmisc)

dat2$age.cat <- cut2(dat2$age,g=5)
table(dat2$age.cat)

dat2$age.cat2 <- cut2(dat2$age,c(25,40,55,70,85))
table(dat2$age.cat2)

```

`cut2` typically needs two arguments.  The first is a numeric variable to convert into a factor, and the second is how to do the splitting.  Specifying `g=5` (as above for `age.cat`) breaks the numeric variable into 5 groups, with the cut points determined by attempting to make the groups as equally sized as possible.  As you can see in this example, due to the odd number of patients, they are not perfectly even.  The second approach requires passing the cut points.  In the second example, we tell `R` to cut the data at 25, 40,....  This results in 6 groups for five cut points.


### Student Question 3: 

> a) Create a new variable in the `dat2` data frame called `sofa.cat` made up of four (aprox.) equally sized groups for SOFA.  Print the sample size in each group.  Why might the number in each group differ so much?
> b) For each SOFA group calculate the number of people who survived and died in the hospital and at 28 days (use the `hosp_exp_flg` and `day_28_flg` variable).
> c) Does mortality go up or down as SOFA increases?
> d) Can you suggest a reason why we might want to convert `age` to a categorical variable instead of using it in it's current form?

a
```{r}
dat2$sofa.cat<-cut2(dat2$sofa_first,g=4)
table(dat2$sofa.cat)
```
the difference of the number in each group is because sofa is dicotomous data and a certain number of patients are in the same degree.
b.
```{r}
table(dat2$sofa.cat,dat2$hosp_exp_flg,dnn=c("sofa","survival"))
table(dat2$sofa.cat,dat2$day_28_flg,dnn=c("sofa","28d_survival"))
```
c.
```{r}
prop.table(table(dat2$sofa.cat,dat2$hosp_exp_flg,dnn=c("sofa","survival")),1)
plot_prop_by_level(dat2,"sofa.cat","hosp_exp_flg")
```

```{r}
prop.table(table(dat2$sofa.cat,dat2$day_28_flg,dnn=c("sofa","28d_survival")),1)
plot_prop_by_level(dat2,"sofa.cat","day_28_flg")
```
there seem significant difference between them.
d.
categorization makes it easier to see the difference in genertion.

### Student Answer 3:

Answer here.

```{r include=FALSE}
dat2$sofa.cat <- cut2(dat2$sofa_first,g=4)
table(dat2$sofa.cat)
table(dat2$sofa.cat,dat2$hosp_exp_flg,dnn=c("SOFA","Hospital Mortality"))
prop.table(table(dat2$sofa.cat,dat2$hosp_exp_flg,dnn=c("SOFA","Hospital Mortality")),1)

table(dat2$sofa.cat,dat2$hosp_exp_flg,dnn=c("SOFA","Hospital Mortality"))
prop.table(table(dat2$sofa.cat,dat2$day_28_flg,dnn=c("SOFA","28 Day Mortality")),1)
```

# Plotting relationships with discrete variables

Plotting discrete data can be a little tricky, but if done right can be very effective.
For an example of why it's difficult, using the basic plot command from last week, let's plot two discrete variables: `gender_num` and `aline_flg`.

```{r}
plot(dat2$gender_num,dat2$aline_flg,xlab="Gender",ylab="IAC")
```

Because we have converted `gender_num` and `aline_flg` to a factor, `R` gives us what is called a "Factor Plot". The area of the light grey region is proportional to the proportion of each gender who received an aline.  In this case, there is not that big of a difference between the genders.

This factor plot is more useful than if we were to keep the original numerical class both these variables had.  For example, if we use the original `dat` data frame both `gender_num` and `aline_flg` are numeric variables, and when we plot these, we don't end up with something very useful:

```{r}
plot(dat$gender_num,dat$aline_flg,xlab="Gender",ylab="IAC")
```

In this case we only have four different types of data points, and although we could try to jitter the values to get a slightly more useful plot, it's unlikely that this would give us a good visual interpretation of the data.

Sometimes the covariate may take on more than two levels.  Here, we plot the in-hospital mortality rate by the different SOFA values, and put a smooth curve through the points.  This covers a more *advanced* topic, and we _*don't*_ expect you to understand the technical details of the code below.


```{r}
plot(names(table(dat2$sofa_first)),sapply(split(dat2,dat2$sofa_first),function(x) { mean(x$hosp_exp_flg==1,na.rm=T)}),xlab="SOFA",ylab="In-Hospital Mortality",cex=log(as.numeric(table(dat2$sofa_first)+1))/5)
lines(smooth.spline(dat2$sofa_first,dat2$hosp_exp_flg==1),type="l")
```

SOFA is a validated disease severity scale for the ICU, and generally correlates strongly with mortality.  Here, while the mortality rate generally increases as SOFA increases, the smooth fit isn't necessarily non-decreasing as SOFA values increase.  We have added points roughly proportional to the sample size of each SOFA level, and you'll see towards the high levels of SOFA, very few patients are observed, with the second highest score (13) having a 100% *survival* rate -- although only in two patients!

For binary outcomes, it is often useful to plot the proportion of patients with the outcome (e.g., mortality rate) by the different levels of a covariate of interest.  Because sample size plays such an important role in the uncertainty associated with these estimate proportions, it seems appropriate to include an estimate of our uncertainty via a confidence interval.

In the `MIMICbook` package you installed above, there is a `plot_prop_by_level` which can plot the proportion of patients with an outcome by one or two factor variables.  For instance, if we wished to plot the in hospital mortality rate by the SOFA categories (`sofa.cat`) we defined above, we can using:


```{r}
plot_prop_by_level(dat2,"sofa.cat","hosp_exp_flg")
```


Often it's useful to consider more than one covariate at a time to assess confounding and effect modification.  Here, if we wished to examine `sofa.cat` and `gender_num` at the same time, we add `factor.var2="gender_num"` to our previous use of `plot_prop_by_level`.

```{r}
plot_prop_by_level(dat2,"sofa.cat","hosp_exp_flg",factor.var2="gender_num")

```

Here we see that in hospital mortality is higher in women in 3/4 of the SOFA groups we considered, suggesting that it might be an important confounder for this outcome and variable.

### Student Question 4:

> a) Make a factor plot of the categories of SOFA we created (`sofa.cat`) and hospital mortality (`hosp_exp_flg`).  Discuss what trend you observed.
> b) Use `plot_prop_by_level` using `sofa.cat` as the covariate of interest and 28 day mortality (`day_28_flg`) as the outcome.  How does this plot compare with part a)?
> c) Include the main covariate of interest for this study `aline_flg` as the second factor variable and extend part b).
> d) Repeat part c), but swap the aline and sofa arguments.  Does this plot better illustrate our objective?  Why or Why not?
> e) Create a new variable, `sofa.cat2`, with cut points at 3, 6, 9, 12.  Repeat parts b) and c).  Is there a problem with these plots?  Why is this case?  Suggest a possible remedy.
> f) Make a plot of the 28 day mortality outcome, `aline_flg` and `chf_flg`.  Ignoring the statistical significance (i.e., do not perform any formal tests), comment on why this plot may suggest the complexity of any potential effect of aline.

a
```{r}
plot_prop_by_level(dat2,"sofa.cat","hosp_exp_flg")
```
Those who in highest quadrant shows higher mortality.

b.
```{r}
plot_prop_by_level(dat2,"sofa.cat","day_28_flg")
```
difference is more appearent in 28 day mortality.

c.
```{r}
plot_prop_by_level(dat2,"sofa.cat","day_28_flg", factor.var2="aline_flg")
```
d.
```{r}
plot_prop_by_level(dat2,"aline_flg","day_28_flg", factor.var2="sofa.cat")
```
it's a bit difficult to find the coressponding data in this setting.
e.
```{r}
dat2$sofa.cat2<-cut2(dat2$sofa_first,c(3,6,9,12))
table(dat2$sofa.cat2)

plot_prop_by_level(dat2,"sofa.cat2","day_28_flg")

plot_prop_by_level(dat2,"sofa.cat2","day_28_flg", factor.var2="aline_flg")
```
the variable of the Q5 is too big and wa cant compare these data precisely. combine Qand Q5 would be a remedy.

f.
```{r}
plot(dat2$day_28_flg,dat2$aline_flg,xlab="28",ylab="aline")
plot(dat2$day_28_flg,dat2$chf_flg,xlab="28",ylab="chf")
plot(dat2$chf_flg,dat2$aline_flg,xlab="chf",ylab="aline")
```

```{r}
plot_prop_by_level(dat2,"chf_flg","day_28_flg", factor.var2="aline_flg")
```
there seem significant difference in A line insertion and CHF.


### Student Answer 4:

Answer here.

```{r include=FALSE}
plot(dat2$sofa.cat,dat2$hosp_exp_flg,xlab="SOFA",ylab="Hospital Mortality") #a

plot_prop_by_level(dat2,"sofa.cat","day_28_flg") #b

plot_prop_by_level(dat2,"sofa.cat","day_28_flg",factor.var2="aline_flg") #c

plot_prop_by_level(dat2,"aline_flg","day_28_flg",factor.var2="sofa.cat") #d

dat2$sofa.cat2 <- cut2(dat2$sofa_first,c(3,6,9,12)) #e
plot_prop_by_level(dat2,"sofa.cat2","day_28_flg")
plot_prop_by_level(dat2,"sofa.cat2","day_28_flg",factor.var2="aline_flg")

plot_prop_by_level(dat2,"chf_flg","day_28_flg", factor.var2="aline_flg") #f

```

## Odds ratios

*Note: For those with a programming background, `R` indexes vectors starting from 1.*

As discussed earlier today, odds ratios are very commonly used to communicate relative effect sizes for binary outcomes, particularly in observational data.  Calculation is straightforward, but often misunderstood.  We start with a 2 x 2 table.  Below is the 2 x 2 table for in hospital mortality and having an arterial line.  I've assigned it to a new variable called `egtab`.

```{r}
egtab <- table(dat2$aline_flg,dat2$hosp_exp_flg,dnn=c("Aline","Hosp. Mort"))
egtab
```

It's hard to interpret the raw counts, so we'll use `prop.table` to compute the proportions who died and lived by row (margin 1, aline).

```{r}
pegtab <- prop.table(egtab,1)
pegtab
```

Odds are $\frac{p}{1-p}$ where $p$ is the proportion with the outcome (death) in a group of patients, which is in the second column.  We can index the above table by column (`tab[,idx]` will retrieve column `idx` from the table [or matrix] `tab`)  to compute the odds in each group.

```{r}
Oddsegtab <-pegtab[,2]/pegtab[,1]
Oddsegtab
```

Now we have the odds of the outcome in those who got an aline `1` and those who didn't `0`.  We need to pick a reference group.  We'll calculate it both ways, but let's assume we want those without an aline to be the reference:

```{r}
Oddsegtab[2]/Oddsegtab[1]
```

If we wanted those with an aline to be the reference group:

```{r}
Oddsegtab[1]/Oddsegtab[2]
```

If we wanted to plot this information, and include a confidence interval, we can use the `plot_OR_by_level` from the `MIMICbook` package:

```{r}
plot_OR_by_level(dat2,"aline_flg","hosp_exp_flg")
```

This by default includes an odds ratio of 1 indicating the reference group.  To remove this point use the `include.ref.group.effect` argument:

```{r}
plot_OR_by_level(dat2,"aline_flg","hosp_exp_flg",include.ref.group.effect = FALSE)
```

You can also look at more than one covariate at a time.  For instance, looking at `aline_flg` and the `gender_num` variable:

```{r}
plot_OR_by_level(dat2,"gender_num","hosp_exp_flg",factor.var2="aline_flg",include.ref.group.effect = TRUE)
```

Here we have computed the odds ratio for aline (vs no aline) separately for men and women.

### Student Question 5:

> a) Create a 2 x 2 table with `chf_flg` and the variable `day_28_flg` outcome and assign it to a variable called `tab22`
> b) Compute the odds ratio for having CHF vs. not having CHF using this table.
> c) Construct a plot of the odds ratios and 95\% confidence intervals using the `plot_OR_by_level` function for CHF and 28 day mortality.
> d) Create a 4 x 2 table with the `sofa.cat` variable and the `day_28_flg` outcome and assign it to a variable called `tab42`.
> e) Pick and define a reference group for `sofa.cat`, and compute the odds ratio(s) for the other levels of `sofa.cat` using `day_28_flg` as your outcome.
> f) Construct a plot of the odds ratios and 95\% confidence intervals using the `plot_OR_by_level` function for the SOFA categories and 28 day mortality.  Make sure the reference groups are the same as parts d) and e).  Look into the `relevel` function in `R` or the `ref.group` argument in the `plot_OR_by_level` function.
> g) Construct a plot looking at the 28 day mortality outcome, and the two variables we considered here, `sofa.cat` and `chf_flg`.  Exchange the variables assigned to the factor.var1 and factor.var2 arguments, and discuss briefly two reasons why you prefer one plot over the other, and what you would conclude from your chosen plot.

a.
```{r}
tab22<-table(dat2$chf_flg,dat2$day_28_flg,dnn=c("chf","28dMort"))
tab22
```

b
```{r}
ptab22<-prop.table(tab22,1)
ptab22
Oddsptab22<-ptab22[,2]/ptab22[,1]
Oddsptab22
Oddsptab22[2]/Oddsptab22[1]
```

```{r}
tab22[1,1]*tab22[2,2]/(tab22[1,2]*tab22[2,1])
```

c.
```{r}
plot_OR_by_level(dat2,"chf_flg","day_28_flg")
```

d.
```{r}
tab42<-table(dat2$sofa.cat,dat2$day_28_flg,dnn=c("sofa","28dMort"))
tab42
             
```

e.
```{r}
ptab42<-prop.table(tab42,1)
ptab42
Oddsptab42<-ptab42[,2]/ptab42[,1]
Oddsptab42
Oddsptab42[2]/Oddsptab42[1]
Oddsptab42[3]/Oddsptab42[1]
Oddsptab42[4]/Oddsptab42[1]
```

f.
```{r}
plot_OR_by_level(dat2,"sofa.cat","day_28_flg",ref.group = 1)
```
g.
```{r}
plot_OR_by_level(dat2,"sofa.cat","day_28_flg",factor.var2="chf_flg")
plot_OR_by_level(dat2,"chf_flg","day_28_flg",factor.var2="sofa.cat")
```
we can see the effect modification by sofa score in CHF history and 28days mortarity.






```{r include=FALSE}
tab22 <- table(dat2$chf_flg,dat2$day_28_flg) #a
tab22[1,1]*tab22[2,2]/(tab22[1,2]*tab22[2,1]) #b
plot_OR_by_level(dat2,"chf_flg","day_28_flg") #c



tab42 <- table(dat2$sofa.cat,dat2$day_28_flg) #d
ptab42 <- prop.table(tab42,1) #e
odds.ptab42 <- ptab42[,2]/ptab42[,1]
odds.ptab42[2:4]/odds.ptab42[1]
plot_OR_by_level(dat2,"sofa.cat","day_28_flg") #f


plot_OR_by_level(dat2,"chf_flg","day_28_flg",factor.var2 = "sofa.cat") #g
plot_OR_by_level(dat2,"sofa.cat","day_28_flg",factor.var2 = "chf_flg") #g

```


### Students Answers 5:

Answer here.